# -*- coding: utf-8 -*-
"""ML Project - 214034C

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NxsxOZxMKbrCMDLYEOobAcwMdMdt6Pxs
"""

import pandas as pd

df = pd.read_csv("sri_lankan_news_article_data.csv")

df.head()
df.info()
df.isnull().sum()
df['industry'].value_counts()

df.shape

df.columns
df.info()

df.shape
df.isnull().sum()

df = df.drop_duplicates()
df.shape

df = df.drop_duplicates(subset=['combined_text'])
df.shape

df['sentiment'].value_counts()

df['industry'].value_counts()

df['combined_text'].isnull().sum()

df[df['combined_text'].str.strip() == ""].shape

df[df['combined_text'] == "."].shape

df = df.drop_duplicates()
df.shape

df = df[df['industry'] != "Here's the list of industries for the news items you've provided, using the expanded list of categories:"]
df.shape

industry_mapping = {
    # Financial
    "Banks": "Financial Services",
    "Diversified Financials": "Financial Services",
    "Insurance": "Financial Services",

    # Food
    "Food Products": "Food Industry",
    "Food, Beverage & Tobacco": "Food Industry",
    "Food & Staples Retailing": "Food Industry",

    # Real Estate
    "Real Estate Management & Development": "Real Estate",
    "Real Estate Management&Development": "Real Estate",

    # Energy
    "Energy": "Energy & Utilities",
    "Utilities": "Energy & Utilities",

    # Consumer
    "Consumer Services": "Consumer Sector",
    "Consumer Goods Industry": "Consumer Sector",
    "Consumer Durables & Apparel": "Consumer Sector",
    "Consumer Discretionary": "Consumer Sector",
    "Household & Personal Products": "Consumer Sector",
    "Retailing": "Consumer Sector",

    # Agriculture
    "Agricultural Raw Materials": "Agriculture"
}

df["industry_clean"] = df["industry"].replace(industry_mapping)

df["industry_clean"].value_counts()

additional_merges = {
    "Health Care Equipment & Services": "Commercial & Professional Services",
    "Software & Services": "Commercial & Professional Services",
    "Telecommunication Services": "Commercial & Professional Services",

    "Real Estate": "Capital Goods",
    "Automobiles & Components": "Capital Goods",
    "Materials": "Capital Goods"
}

df["industry_clean"] = df["industry_clean"].replace(additional_merges)

df["industry_clean"].value_counts()

# Combine headline and news description
df['headline_desc'] = df['Headline'] + " " + df['News Description'].fillna('')

# Check sample
df[['Headline','News Description','headline_desc']].head()

import re
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('english')) - {"not", "no", "never"}

def clean_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation & special chars
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

df['clean_text'] = df['headline_desc'].apply(clean_text)
df[['headline_desc', 'clean_text']].head()

# Use the cleaned text from the full dataset
df_sentiment = df.copy()  # Use your original df after text cleaning but before industry downsampling

# Keep only POSITIVE/NEGATIVE for sentiment
df_sentiment = df_sentiment[df_sentiment['sentiment'].isin(['POSITIVE', 'NEGATIVE'])].reset_index(drop=True)

# TF-IDF features (use same preprocessing as before)
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000, min_df=3, ngram_range=(1,3))
X_text_sent = tfidf.fit_transform(df_sentiment['clean_text'])

# Optional: include metadata if desired
from scipy.sparse import hstack
X_metadata_sent = pd.get_dummies(df_sentiment['Related Field'])
X = hstack([X_text_sent, X_metadata_sent.values])

# # Encode sentiment labels
# from sklearn.preprocessing import LabelEncoder
# le_sent = LabelEncoder()
# y_sent = le_sent.fit_transform(df_sentiment['sentiment'])  # NEGATIVE=0, POSITIVE=1

"""Testing"""

# Use TF-IDF + metadata consistently
from scipy.sparse import hstack

# TF-IDF
tfidf = TfidfVectorizer(max_features=5000, min_df=3, ngram_range=(1,3))
X_text = tfidf.fit_transform(df_sentiment['clean_text'])

# Metadata
X_metadata = pd.get_dummies(df_sentiment['Related Field'])

# Stack features
X_full_sent = hstack([X_text, X_metadata.values])

# Encode labels
from sklearn.preprocessing import LabelEncoder
le_sentiment = LabelEncoder()
y_sentiment = le_sentiment.fit_transform(df_sentiment['sentiment'])

# Split train/val/test
from sklearn.model_selection import train_test_split
X_train_sent, X_temp_sent, y_train_sent, y_temp_sent = train_test_split(
    X_full_sent, y_sentiment, test_size=0.3, random_state=42, stratify=y_sentiment
)
X_val_sent, X_test_sent, y_val_sent, y_test_sent = train_test_split(
    X_temp_sent, y_temp_sent, test_size=0.5, random_state=42, stratify=y_temp_sent
)

df_sentiment['sentiment'].value_counts()

# One-hot encode 'Related Field'
df_metadata = pd.get_dummies(df['Related Field'], prefix='related')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from scipy.sparse import hstack
import pandas as pd

# TF-IDF for combined_text + headline
tfidf_text = TfidfVectorizer(max_features=4000, min_df=3, ngram_range=(1,3))
X_text_combined = tfidf_text.fit_transform(df['headline_desc'] + " " + df['combined_text'])

# One-hot encode Related Field
ohe = OneHotEncoder(sparse_output=True)  # <-- corrected parameter
X_related = ohe.fit_transform(df[['Related Field']])

# Optional: extract month/day-of-week from Date
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
X_date = pd.get_dummies(df['Date'].dt.month, prefix='month', sparse=True)
X_day = pd.get_dummies(df['Date'].dt.dayofweek, prefix='dow', sparse=True)

# Stack all features together
X_full = hstack([X_text_combined, X_related, X_date, X_day])
print("Final feature shape:", X_full.shape)

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# # y_sent = df['sentiment']
# X_train_sent, X_val_sent, y_train_sent, y_val_sent = train_test_split(
#     X, y_sent, test_size=0.2, random_state=42, stratify=y_sent
# )
# X_train, X_val, y_train, y_val = train_test_split(X_full, y_sent, test_size=0.2, random_state=42, stratify=y_sent)

# sm = SMOTE(random_state=42)
# X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report

svm_sent = LinearSVC(class_weight='balanced', max_iter=5000, random_state=42)
svm_sent.fit(X_train_sent, y_train_sent)

y_val_pred_sent = svm_sent.predict(X_val_sent)

print("Validation Accuracy (Sentiment - SVM):", accuracy_score(y_val_sent, y_val_pred_sent))
print(classification_report(y_val_sent, y_val_pred_sent, target_names=le_sent.classes_))

# Convert Date to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Extract month and day-of-week as features
df['month'] = df['Date'].dt.month
df['day_of_week'] = df['Date'].dt.dayofweek

# One-hot encode month and day_of_week
df_metadata = pd.concat([
    df_metadata,
    pd.get_dummies(df['month'], prefix='month'),
    pd.get_dummies(df['day_of_week'], prefix='dow')
], axis=1)

# Separate Other and non-Other
df_other = df[df["industry_clean"] == "Other"]
df_non_other = df[df["industry_clean"] != "Other"]

# Downsample Other to 600
df_other_downsampled = df_other.sample(n=200, random_state=42)

# Combine back
df_balanced = pd.concat([df_non_other, df_other_downsampled])

# Shuffle dataset
df_balanced = df_balanced.sample(frac=1, random_state=42)

df_balanced["industry_clean"].value_counts()

df = df_balanced.copy()
df.shape

# Check for empty clean_text
empty_rows = df[df['clean_text'].str.strip() == '']
empty_rows.shape

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=5000,
    min_df=3,
    ngram_range=(1,3)
)
X_text = tfidf.fit_transform(df['clean_text'])

# Recreate metadata after final cleaned df
df_metadata = pd.get_dummies(df['Related Field'], prefix='related')
df_metadata = pd.concat([
    df_metadata,
    pd.get_dummies(df['Date'].dt.month, prefix='month'),
    pd.get_dummies(df['Date'].dt.dayofweek, prefix='dow')
], axis=1)

# Check shapes
print("TF-IDF shape:", X_text.shape)
print("Metadata shape:", df_metadata.shape)

from scipy.sparse import hstack

X = hstack([X_text, df_metadata.values])
y_ind = df['industry_clean']
y_sent = df['sentiment']

from sklearn.preprocessing import LabelEncoder

le_sentiment = LabelEncoder()
y_sentiment = le_sentiment.fit_transform(df['sentiment'])  # NEGATIVE=0, POSITIVE=1

from sklearn.preprocessing import LabelEncoder

# Encode industry
le_industry = LabelEncoder()
y_industry = le_industry.fit_transform(df['industry_clean'])

# Encode sentiment
le_sentiment = LabelEncoder()
y_sentiment = le_sentiment.fit_transform(df['sentiment'])

print("Industry classes:", len(le_industry.classes_))
print("Sentiment classes:", le_sentiment.classes_)

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Oversample only the INDUSTRY feature (multi-class)
X_bal, y_ind_bal = smote.fit_resample(X, y_ind)

# Check new class distribution
from collections import Counter
print("Original class counts:\n", Counter(y_ind))
print("Balanced class counts:\n", Counter(y_ind_bal))

# from sklearn.model_selection import train_test_split

# # Split: 70% train, 15% validation, 15% test
# X_train, X_temp, y_train, y_temp = train_test_split(
#     X_bal, y_ind_bal, test_size=0.30, random_state=42, stratify=y_ind_bal
# )

# X_val, X_test, y_val, y_test = train_test_split(
#     X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
# )

# print("Train shape:", X_train.shape)
# print("Validation shape:", X_val.shape)
# print("Test shape:", X_test.shape)

from sklearn.model_selection import train_test_split

# Split: 70% train, 15% validation, 15% test
X_train_ind, X_temp_ind, y_train_ind, y_temp_ind = train_test_split(
    X_bal,          # Features (TF-IDF + optional metadata)
    y_ind_bal,      # Industry labels
    test_size=0.3,
    random_state=42,
    stratify=y_ind_bal
)

X_val_ind, X_test_ind, y_val_ind, y_test_ind = train_test_split(
    X_temp_ind,
    y_temp_ind,
    test_size=0.5,
    random_state=42,
    stratify=y_temp_ind
)

print("Industry Train shape:", X_train_ind.shape)
print("Industry Validation shape:", X_val_ind.shape)
print("Industry Test shape:", X_test_ind.shape)

# Encode sentiment
from sklearn.preprocessing import LabelEncoder
le_sentiment = LabelEncoder()
y_sentiment = le_sentiment.fit_transform(df['sentiment'])  # NEGATIVE=0, POSITIVE=1

# Split: 70% train, 15% validation, 15% test
X_train_sent, X_temp_sent, y_train_sent, y_temp_sent = train_test_split(
    X_text,         # Features: TF-IDF only (or TF-IDF + metadata if available)
    y_sentiment,
    test_size=0.3,
    random_state=42,
    stratify=y_sentiment
)

X_val_sent, X_test_sent, y_val_sent, y_test_sent = train_test_split(
    X_temp_sent,
    y_temp_sent,
    test_size=0.5,
    random_state=42,
    stratify=y_temp_sent
)

print("Sentiment Train shape:", X_train_sent.shape)
print("Sentiment Validation shape:", X_val_sent.shape)
print("Sentiment Test shape:", X_test_sent.shape)

# from sklearn.model_selection import train_test_split

# # First split: Train (70%) and Temp (30%)
# X_train, X_temp, y_ind_train, y_ind_temp, y_sent_train, y_sent_temp = train_test_split(
#     X_text,
#     y_industry,
#     y_sentiment,
#     test_size=0.3,
#     random_state=42,
#     stratify=y_industry
# )

# # Second split: Validation (15%) and Test (15%)
# X_val, X_test, y_ind_val, y_ind_test, y_sent_val, y_sent_test = train_test_split(
#     X_temp,
#     y_ind_temp,
#     y_sent_temp,
#     test_size=0.5,
#     random_state=42,
#     stratify=y_ind_temp
# )

# print("Train shape:", X_train.shape)
# print("Validation shape:", X_val.shape)
# print("Test shape:", X_test.shape)

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

# Initialize SVM with class weighting
svm_model = LinearSVC(class_weight='balanced', random_state=42, max_iter=5000)

# Train on industry training set
svm_model.fit(X_train_ind, y_train_ind)

# Predict on industry validation set
y_val_pred_ind = svm_model.predict(X_val_ind)

# Evaluate
val_acc_ind = accuracy_score(y_val_ind, y_val_pred_ind)
print("Validation Accuracy (Industry - Balanced SVM):", val_acc_ind)
print("\nClassification Report (Industry):\n")
print(classification_report(y_val_ind, y_val_pred_ind))

"""**Model Evaluation**"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on test set
y_test_pred_ind = svm_model.predict(X_test_ind)

# Accuracy
acc_ind_test = accuracy_score(y_test_ind, y_test_pred_ind)
print("Test Accuracy (Industry):", acc_ind_test)

# Classification report
print(classification_report(y_test_ind, y_test_pred_ind, target_names=le_industry.classes_))

# Confusion matrix
cm_ind = confusion_matrix(y_test_ind, y_test_pred_ind)
plt.figure(figsize=(10,7))
sns.heatmap(cm_ind, annot=True, fmt="d", xticklabels=le_industry.classes_, yticklabels=le_industry.classes_)
plt.title("Industry Prediction - Confusion Matrix")
plt.show()

# Predict on validation
y_val_pred_ind = svm_model.predict(X_val_ind)
val_acc_ind = accuracy_score(y_val_ind, y_val_pred_ind)
print("\nValidation Accuracy (Industry):", val_acc_ind)
print("\nClassification Report (Industry - Validation):")
print(classification_report(y_val_ind, y_val_pred_ind, target_names=le_industry.classes_))

# Predict on test set
y_test_pred_ind = svm_model.predict(X_test_ind)
test_acc_ind = accuracy_score(y_test_ind, y_test_pred_ind)
print("\nTest Accuracy (Industry):", test_acc_ind)
print("\nClassification Report (Industry - Test):")
print(classification_report(y_test_ind, y_test_pred_ind, target_names=le_industry.classes_))

# Confusion Matrix
cm_ind = confusion_matrix(y_test_ind, y_test_pred_ind)
disp_ind = ConfusionMatrixDisplay(confusion_matrix=cm_ind, display_labels=le_industry.classes_)
disp_ind.plot(cmap='Oranges', xticks_rotation='vertical')

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Predict on validation
y_val_pred_sent = svm_sent.predict(X_val_sent)
print("Validation Accuracy (Sentiment):", accuracy_score(y_val_sent, y_val_pred_sent))
print("\nClassification Report (Sentiment - Validation):")
print(classification_report(y_val_sent, y_val_pred_sent, target_names=le_sentiment.classes_))

# Predict on test set
y_test_pred_sent = svm_sent.predict(X_test_sent)
acc_sent_test = accuracy_score(y_test_sent, y_test_pred_sent)
print("\nTest Accuracy (Sentiment):", acc_sent_test)
print("\nClassification Report (Sentiment - Test):")
print(classification_report(y_test_sent, y_test_pred_sent, target_names=le_sentiment.classes_))

# Confusion Matrix
cm_sent = confusion_matrix(y_test_sent, y_test_pred_sent)
disp_sent = ConfusionMatrixDisplay(confusion_matrix=cm_sent, display_labels=le_sentiment.classes_)
disp_sent.plot(cmap='Blues', xticks_rotation='vertical')

"""**Explainability & Interpretation**"""

import numpy as np
feature_names = tfidf.get_feature_names_out()
coef = svm_sent.coef_[0]  # for binary sentiment
top_pos_idx = np.argsort(coef)[-20:]
top_neg_idx = np.argsort(coef)[:20]

print("Top Positive Words:", feature_names[top_pos_idx])
print("Top Negative Words:", feature_names[top_neg_idx])

plt.figure(figsize=(10,5))
plt.barh(feature_names[top_pos_idx], coef[top_pos_idx], color='green')
plt.barh(feature_names[top_neg_idx], coef[top_neg_idx], color='red')
plt.title("Top Features for Sentiment Prediction")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Combine TF-IDF + metadata feature names
feature_names_text = tfidf.get_feature_names_out()          # TF-IDF
feature_names_meta = df_metadata.columns.tolist()           # metadata columns
feature_names = np.concatenate([feature_names_text, feature_names_meta])

# Plot top 20 features for each industry
for i, class_name in enumerate(le_industry.classes_):
    coefs = svm_model.coef_[i]
    top_idx = np.argsort(coefs)[-20:]  # top 20 positive features
    plt.figure(figsize=(10,6))
    plt.barh(range(len(top_idx)), coefs[top_idx])
    plt.yticks(range(len(top_idx)), feature_names[top_idx])
    plt.title(f"Top Features for Industry: {class_name}")
    plt.xlabel("Coefficient Value")
    plt.show()

import shap

# Use a smaller sample to speed up SHAP
X_sample = X_test_sent[:100].toarray()
explainer = shap.LinearExplainer(svm_sent, X_train_sent.toarray())
shap_values = explainer.shap_values(X_sample)

# Summary plot
shap.summary_plot(shap_values, X_sample, feature_names=tfidf.get_feature_names_out())